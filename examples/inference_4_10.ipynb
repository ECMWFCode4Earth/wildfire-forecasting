{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 days FWI Reanalysis forecast using 4 days input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do 10 day FWI-Reanalysis forecast using 4-days input of Temperature, Relative humidity, Total precipitation, and Wind speed. The sample data must span for at least 13 consecutive days. The file format of the stored data should be in netCDF4 format. A pretrained model checkpoint conrresponding to the input and forecast timespan will be required as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing working directory to import modules naturally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../src')\n",
    "from train import str2num, get_hparams, get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing installed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functionality\n",
    "import random\n",
    "from glob import glob\n",
    "from argparse import Namespace\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "# Keep the execution uncomplicated\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ploting purposes\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "# Helper modules\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensuring reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2334\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Data\n",
    "The next cell downloads the required data for performing inference using 2 (4) days weather forcings for 1 (10) days FWI prediction. The data is stored in `./deepfwi-mini-sample/fwi-forcings` and `./deepfwi-mini-sample/fwi-reanalysis`.\n",
    "\n",
    "If you already have the data downloaded, comment the cell below and please place it in the directory structure explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../examples')\n",
    "!gsutil cp -r gs://deepfwi-mini-sample .\n",
    "os.chdir('../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating pickled list of test-set files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the files needed for output variable are specified.\n",
    "# Input varaible files for corresponding dates are auto-deduced.\n",
    "test_out_files = [f'/nvme0/fwi-reanalysis/ECMWF_FWI_201904{x:02}_1200_hr_fwi_e5.nc' for x in range(1, 14)]\n",
    "print(\"First few files...\", *test_out_files[:3], sep='\\n')\n",
    "\n",
    "# Pickling the list which can be used later\n",
    "with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "    pickle.dump(test_out_files, tmp)\n",
    "    test_set_path = tmp.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting the knobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing hyperparamters and configuration in a dictionary\n",
    "hparams_dict = get_hparams(\n",
    "    # Feature density of U-Net layers\n",
    "    init_features=16,\n",
    "    # Number of input channels (=4*input days)\n",
    "    in_days=4,\n",
    "    # Number of prediction channels (=output days)\n",
    "    out_days=10,\n",
    "    # Loss metric\n",
    "    loss='mse',\n",
    "    # Batch size\n",
    "    batch_size=1,\n",
    "    # Number of GPUs to use\n",
    "    gpus=1,\n",
    "    # Turn on temporal and spatial constraints for case-study in Australia\n",
    "    case_study=False,\n",
    "    # Whether to ignore fire-prone regions\n",
    "    clip_fwi=False,\n",
    "    # Pickled list of output files in test-set\n",
    "    test_set=test_set_path,\n",
    "    # Model architecture\n",
    "    model='unet_tapered',\n",
    "    # Output dataloader\n",
    "    out='fwi_reanalysis',\n",
    "    # Directory with FWI Forcings input\n",
    "    forcings_dir='/nvme1/fwi-forcings',\n",
    "    # Directory with FWI Renanlysis output\n",
    "    reanalysis_dir='/nvme0/fwi-reanalysis',\n",
    "    # Custom mask stored as numpy array\n",
    "    mask='dataloader/mask.npy',\n",
    "    # Model checkpoint file used to load the pretrained weights\n",
    "    checkpoint_file='/w/deepfwi/src/model/checkpoints/pre_trained/5/4_10/epoch_75_82.ckpt'\n",
    "    )\n",
    "\n",
    "# Converting the dictionary to Namespace for easier access\n",
    "hparams = Namespace(**hparams_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the flag for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.eval = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model architecture and attach with the data\n",
    "model = get_model(hparams)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model.load_state_dict(torch.load(hparams.checkpoint_file)[\"state_dict\"])\n",
    "\n",
    "# Turn off the gradients\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor\n",
    "x = model.data[0][0].unsqueeze(0)\n",
    "\n",
    "# Ground truth tensor\n",
    "y = model.data[0][1].unsqueeze(0)\n",
    "\n",
    "# Predicted tensor\n",
    "y_hat = model(x).detach()\n",
    "\n",
    "# Masking the prediction as done with the ground truth\n",
    "y_hat[torch.isnan(y)] = torch.tensor(float('nan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20, hparams.in_days*4))\n",
    "fig.suptitle('Input FWI-variables from 2019-04-01 till 2019-04-04', fontsize=25)\n",
    "for i in range(hparams.in_days):\n",
    "    for j in range(4):\n",
    "        ax = fig.add_subplot(hparams.in_days, 4, 4*i+j+1)\n",
    "        if i==0:\n",
    "            if j==0:\n",
    "                ax.set_title('Relative Humidity',fontsize='23', pad=20)\n",
    "            elif j==1:\n",
    "                ax.set_title('Temperature',fontsize='23', pad=20)\n",
    "            elif j==2:\n",
    "                ax.set_title('Precipitation',fontsize='23', pad=20)\n",
    "            else:\n",
    "                ax.set_title('Wind speed',fontsize='23', pad=20)\n",
    "        if j==0:\n",
    "            ax.set_ylabel(f'2019-04-{i+1:02}', fontsize='23', labelpad=20)\n",
    "        if j==1:\n",
    "            plt.imshow(x.squeeze()[4*i+j], cmap='gist_ncar')\n",
    "        elif j==2:\n",
    "            plt.imshow(x.squeeze()[4*i+j], cmap='flag')\n",
    "        else:\n",
    "            plt.imshow(x.squeeze()[4*i+j], cmap='hsv')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Helper function to add colorbar in the plots.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(im, title):\n",
    "    fig, ax = plt.subplots(figsize = (20,10))\n",
    "    fig.suptitle(title, fontsize=25)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('bottom', size='5%', pad=0.5)\n",
    "    im = ax.imshow(im, cmap='jet')\n",
    "    fig.colorbar(im, cax=cax, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot(y.squeeze()[i], f'Observed FWI-reanalysis for 2019-04-{4+i:02}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot(y_hat.squeeze()[i], f'Predicted FWI-reanalysis for 2019-04-{4+i:02}\\nAccuracy: \\\n",
    "    {((y-y_hat).squeeze()[i].abs()<9.4)[model.data.mask].float().mean()*100:.2f}%*')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>*Using half of MAD as the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot((y-y_hat).abs().squeeze()[i], f\"Error in predicted FWI-reanalysis for 2019-04-{4+i:02}\\nMAE: \\\n",
    "    {((y-y_hat).squeeze()[i].abs())[model.data.mask].float().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The receptive field size for input stays the same as for global prediction. After getting the global predictions we extract only the values falling within the case-study region coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_case = y[0][:, 355:480, 400:550]\n",
    "y_hat_case = y_hat[0][:, 355:480, 400:550]\n",
    "mask_case = model.data.mask[355:480, 400:550]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot(y_case.squeeze()[i], f'Observed FWI-reanalysis in the case-study region for 2019-04-{4+i:02}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot(y_hat_case.squeeze()[i], f'Predicted FWI-reanalysis in the case-study region for 2019-04-{4+i:02}\\nAccuracy: \\\n",
    "    {((y_case-y_hat_case).squeeze()[i].abs()<9.4)[mask_case].float().mean()*100:.2f}%*')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>*Using half of MAD as the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot((y_case-y_hat_case).abs().squeeze()[i], f\"Error in predicted FWI-reanalysis for 2019-04-{4+i:02}\\nMAE: \\\n",
    "    {((y_case-y_hat_case).squeeze()[i].abs())[mask_case].float().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trained with Box-Cox transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do inference on the model trained with Box-Cox transformed output variable, we would need the corresponding lambda value for the inverse transformation. The chekpoint used here requires the lambda = 0.1182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.boxcox = 0.1182\n",
    "hparams.checkpoint_file='/w/deepfwi/src/model/checkpoints/pre_trained/7/4_10/epoch_99_100.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model architecture and attach with the data\n",
    "model = get_model(hparams)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model.load_state_dict(torch.load(hparams.checkpoint_file)[\"state_dict\"])\n",
    "\n",
    "# Turn off the gradients\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor\n",
    "x = model.data[0][0].unsqueeze(0)\n",
    "\n",
    "# Ground truth tensor\n",
    "y = model.data[0][1].unsqueeze(0)\n",
    "\n",
    "# Predicted tensor\n",
    "y_hat = model(x).detach()\n",
    "y_hat = torch.from_numpy(\n",
    "    inv_boxcox(y_hat.cpu().numpy(), model.hparams.boxcox)\n",
    ")\n",
    "\n",
    "# Masking the prediction as done with the ground truth\n",
    "y_hat[torch.isnan(y)] = torch.tensor(float('nan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot(y_hat.squeeze()[i], f'Predicted FWI-reanalysis for 2019-04-{4+i:02}\\nAccuracy: \\\n",
    "    {((y-y_hat).squeeze()[i].abs()<9.4)[model.data.mask].float().mean()*100:.2f}%*')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>*Using half of MAD as the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(hparams.out_days):\n",
    "    plot((y-y_hat).abs().squeeze()[i], f\"Error in predicted FWI-reanalysis for 2019-04-{4+i:02}\\nMAE: \\\n",
    "    {((y-y_hat).squeeze()[i].abs())[model.data.mask].float().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing inputs manually to the model, the Trainer can be used. It will run the prepared model over the entire data and generate the result metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trainer object responsible for running the model\n",
    "trainer = pl.Trainer(gpus=hparams.gpus)\n",
    "\n",
    "# Running inference with the supplied model\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did 10 days global forecast using 4-day input of Relative humidty, Temperature, Total precipitation, and Wind speed. We also looked at the error distribution of the prediction across the various regions and timescales.\n",
    "\n",
    "| Day | MAE | Accuracy | MSE |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | 3.798 | 88.19% | 44.332 |\n",
    "| 1 | 4.875 | 83.87% | 81.655 |\n",
    "| 2 | 5.042 | 82.79% | 77.965 |\n",
    "| 3 | 4.846 | 83.28% | 72.909 |\n",
    "| 4 | 5.134 | 82.41% | 79.106 |\n",
    "| 5 | 5.310 | 81.93% | 88.908 |\n",
    "| 6 | 5.584 | 81.70% | 123.03 |\n",
    "| 7 | 5.305 | 81.27% | 87.120 |\n",
    "| 8 | 5.596 | 80.84% | 98.665 |\n",
    "| 9 | 5.932 | 80.00% | 114.54 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
